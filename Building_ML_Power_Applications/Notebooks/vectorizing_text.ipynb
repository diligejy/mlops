{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 벡터화하기\n",
    "예상했겠지만 텍스트 벡터화의 목표는 텍스트 입력을 수치 표현으로 변환하는 것입니다. 이상적으로는 이 수치 표현이 머신러닝 모델이 텍스트에 있는 패턴을 쉽게 감지할 수 있도록 의미있는 정보를 나타내야합니다.\n",
    "\n",
    "텍스트를 벡터화하는 방법에는 여러 가지가 있습니다. 여기서는 두 가지를 소개합니다:\n",
    "\n",
    "- TF-IDF는 주어진 말뭉치에서 단어의 상대적인 등장 횟수를 기반으로 문서를 벡터화합니다.\n",
    "- 사전 훈련된 모델로 다른 말뭉치의 정보를 활용합니다.\n",
    "\n",
    "데이터 분할 노트북에서처럼 데이터를 로드하고 훈련 세트와 테스트 세트로 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "issubclass() arg 1 must be a class",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13120\\2682809983.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mumap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jinyoung\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefer_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_cpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_gpu\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jinyoung\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mattributeruler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAttributeRuler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdep_parser\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDependencyParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0medit_tree_lemmatizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEditTreeLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mentity_linker\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEntityLinker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mentityruler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEntityRuler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jinyoung\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\attributeruler.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mErrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatcher\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscorer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mScorer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jinyoung\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer_exceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBASE_EXCEPTIONS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mURL_MATCH\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlookups\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_lookups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpipe_analysis\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0manalyze_pipes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_pipe_analysis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_attrs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m from .schemas import (\n\u001b[0;32m     45\u001b[0m     \u001b[0mConfigSchema\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jinyoung\\anaconda3\\lib\\site-packages\\spacy\\pipe_analysis.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mErrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mToken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdot_to_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jinyoung\\anaconda3\\lib\\site-packages\\spacy\\tokens\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_serialize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDocBin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmorphanalysis\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMorphAnalysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mspan\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSpan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mspan_group\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSpanGroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jinyoung\\anaconda3\\lib\\site-packages\\spacy\\tokens\\_serialize.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mErrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSimpleFrozenList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_dict_proxies\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSpanGroups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDOCBIN_ALL_ATTRS\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mALL_ATTRS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jinyoung\\anaconda3\\lib\\site-packages\\spacy\\vocab.pyx\u001b[0m in \u001b[0;36minit spacy.vocab\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jinyoung\\anaconda3\\lib\\site-packages\\spacy\\tokens\\doc.pyx\u001b[0m in \u001b[0;36minit spacy.tokens.doc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jinyoung\\anaconda3\\lib\\site-packages\\spacy\\schemas.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mTokenPattern\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m     \u001b[0morth\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mStringValue\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mStringValue\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jinyoung\\anaconda3\\lib\\site-packages\\pydantic\\main.cp39-win_amd64.pyd\u001b[0m in \u001b[0;36mpydantic.main.ModelMetaclass.__new__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jinyoung\\anaconda3\\lib\\site-packages\\pydantic\\fields.cp39-win_amd64.pyd\u001b[0m in \u001b[0;36mpydantic.fields.ModelField.infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jinyoung\\anaconda3\\lib\\site-packages\\pydantic\\fields.cp39-win_amd64.pyd\u001b[0m in \u001b[0;36mpydantic.fields.ModelField.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jinyoung\\anaconda3\\lib\\site-packages\\pydantic\\fields.cp39-win_amd64.pyd\u001b[0m in \u001b[0;36mpydantic.fields.ModelField.prepare\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jinyoung\\anaconda3\\lib\\site-packages\\pydantic\\fields.cp39-win_amd64.pyd\u001b[0m in \u001b[0;36mpydantic.fields.ModelField._type_analysis\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jinyoung\\anaconda3\\lib\\site-packages\\pydantic\\fields.cp39-win_amd64.pyd\u001b[0m in \u001b[0;36mpydantic.fields.ModelField._type_analysis\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jinyoung\\anaconda3\\lib\\typing.py\u001b[0m in \u001b[0;36m__subclasscheck__\u001b[1;34m(self, cls)\u001b[0m\n\u001b[0;32m    850\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__origin__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__origin__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_GenericAlias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 852\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__origin__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    853\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__subclasscheck__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: issubclass() arg 1 must be a class"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import umap\n",
    "import numpy as np \n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ml_editor.data_processing import format_raw_df, get_split_by_author\n",
    "from ml_editor.data_visualization import plot_embeddings\n",
    "\n",
    "data_path = Path('../data/writers.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "df = format_raw_df(df.copy())\n",
    "\n",
    "train_author, test_author = get_split_by_author(df[df[\"is_question\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "TF-IDF는 전체 말뭉치와 비교하여 문서에 있는 각 단어의 상대적인 빈도를 기반으로 임베딩을 만듭니다. sklearn을 사용해 TF-IDF를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = train_author[train_author[\"is_question\"]]\n",
    "raw_text = questions[\"body_text\"]\n",
    "# 그래프에 컬러로 표시할 레이블을 추출합니다.\n",
    "# 이 레이블은 분류기를 위한 레이블과 같을 필요는 없습니다.\n",
    "sent_labels = questions[\"AcceptedAnswerId\"].notna()\n",
    "\n",
    "sent_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf 객체를 만듭니다.\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_features=2**21)\n",
    "\n",
    "# 데이터셋에 있는 질문에 적용합니다.\n",
    "# 벡터화된 배열을 반환합니다.\n",
    "bag_of_words = vectorizer.fit_transform(raw_text)\n",
    "\n",
    "bag_of_words.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 텍스트를 벡터화했습니다. 이 과정을 임베딩이라 부르지만 결과 벡터를 임베딩이라고도 합니다. 임베딩 벡터를 PCA, t-SNE, UMAP 등과 같은 차원 축소 기법을 사용해 2차원에 투영하여 시각화할 수 있습니다. 여기서는 UMAP을 사용합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'umap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27924\\1471542460.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mumap_embedder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUMAP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mumap_bow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumap_embedder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbag_of_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'umap' is not defined"
     ]
    }
   ],
   "source": [
    "umap_embedder = umap.UMAP()\n",
    "umap_bow = umap_embedder.fit_transform(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(umap_bow, sent_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 간결하게 표혔했지만 트렌드를 보거나 향후 레이블링을 하는데 도움이 됩니다(실제 벡터는 2차원보다 훨씬 큽니다). 특성을 추가할 때 클래스 분할이 잘 되는지 확인하기 위해 임베딩을 시각화할 가치가 있습니다.\n",
    "\n",
    "## 사전 훈련딘 단어 임베딩\n",
    "두 번째 방법은 매우 큰 말뭉치에서 훈련된 기존 단어 벡터를 로드하여 현재 말뭉치를 넘어서 일반적인 정보를 활용하는 것입니다. spacy 라이브러리를 사용해 이를 수행해 보죠.\n",
    "\n",
    "먼저 터미널에서 다음 명령으로 en_core_web_lg를 다운로드합니다.\n",
    "\n",
    "python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라지 모델을 로드하고 이 작업에 불필요한 요소를 제외합니다.\n",
    "# 이렇게 하면 벡터화 과정의 속도를 크게 높일 수 있습니다.\n",
    "# 모델에 대한 자세한 내용은 https://spacy.io/models/en#en_core_web_lg 을 참고하세요.\n",
    "nlp = spacy.load('en_core_web_lg', disable=[\"parser\", \"tagger\", \"ner\", \"textcat\", \"lemmatizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 질문에 대한 벡터를 얻습니다.\n",
    "# 기본적으로 반환된 벡터는 문장에 있는 모든 벡터의 평균입니다.\n",
    "# 자세한 내용은 https://spacy.io/usage/vectors-similarity 을 참고하세요.\n",
    "spacy_emb = train_author[train_author[\"is_question\"]][\"body_text\"].apply(lambda x: nlp(x).vector)\n",
    "embeddings = np.vstack(spacy_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embedder = umap.UMAP()\n",
    "umap_emb = umap_embedder.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP을 사용해 사전 훈련된 임베딩을 시각화해 보죠.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(umap_emb, sent_labels)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.savefig('../images/bmlpa_figures/ch04-9.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임베딩 방법이 다르기 때문에 이 데이터셋의 위상이 다릅니다. 하지만 내재된 데이터는 동일합니다.\n",
    "\n",
    "임베딩 방법이 다르면 데이터셋을 다르게 표현하게 됩니다. 현재 데이터에서 훈련된 임베딩을 사용할 때 동일한 어휘를 사용한 문서가 가깝게 임베딩될 것입니다. 하지만 다른 말뭉치에서 훈련된 모델을 사용하면 이 말뭉치의 정보를 활용할 수 있습니다. 이런 모델을 사용하면 완전히 다른 어휘를 사용하도라도 의미적으로 비슷한 문장이 서로 가깝게 임베딩될 수 있습니다.\n",
    "\n",
    "데이터 벡터화 방식을 바꾸면 모델 성능에 큰 영향을 끼치는 경우가 종종 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
